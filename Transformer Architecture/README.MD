# Transformer Model from Scratch
This repository contains transformer model that we bulit from scratch from mathematical level on the basis of “Attention Is All You Need” is a research paper by Ashish Vaswani et al. that proposes a new neural network architecture for sequence-to-sequence tasks.

## Dataset Description
* We have taken English and Hindi paralel corpus dataset of of size 1.94GB and it contains corpus has 49.6M sentence pairs between English to Hindi from AI4BHARAT SAMANANTAR.
* AI4Bharat is a non-profit, open-source community of engineers, domain experts, policy makers and academicians, all collaborating to build AI solutions for solving India’s critical socio-economic and environmental challenges.  It is a research lab at IIT Madras which works on developing open-source datasets, tools, models and applications for Indian languages. 
This initiative is helmed by IIT Madras faculty members Mitesh Khapra and Pratyush Kumar, as a part of their technology startup One Fourth Labs.
* The dataset can be found here in this [link](https://drive.google.com/drive/folders/1in3o1e7IkFm9OcQCh3yCOTwDuxgmvesz?usp=sharing)

## Overview of the architecture
<img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="500" height="500">


## Usage

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```
### 2. Install Dependencies
Ensure you have the necessary dependencies installed. You can use the following command to install them:

```bash
pip install -r requirements.txt
```
### 3. Working
* Upload the transformer_model_py_1.py file in Transformer-Trained.ipynb  , incase you futher want to train or finetune the model.
* The rest of the files in Transformer Architecture folder are individually implemented components of transformer architecture for better understanding.

### 5. Customization
Feel free to fine-tune the model further or adjust hyperparameters based on your specific use case. Check the documentation for advanced usage.


## Additional Information
Explore the Transformer-Trained.ipynb notebook for a detailed walkthrough and examples.

## Citation
If you use this dataset or Transformer model in your research or work, kindly cite the source or authors as appropriate.

