{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TiXhhDE9gwcM",
        "outputId": "4dbe7065-3be5-4fde-8328-6e351a820912"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3f5f9808-17a5-41b3-aa03-644702885840\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3f5f9808-17a5-41b3-aa03-644702885840\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving transformer_model_py_1.py to transformer_model_py_1.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wUy_rgPMgzHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e348fdf4-89f0-43e7-b5ef-85d3847fb5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "#from transformer import Transformer\n",
        "from transformer_model_py_1 import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "E1RgJl2Biz-G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n", 
        "# Vocabularies\n",
        "hindi_vocab = [\n",
        "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '।', '॥',\n",
        "    'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ',\n",
        "    'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण',\n",
        "    'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म',\n",
        "    'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह',\n",
        "    'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्', 'ॐ',\n",
        "    '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocab = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                      ':', '<', '=', '>', '?', '@',\n",
        "                      '[', '\\\\', ']', '^', '_', '`',\n",
        "                      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                      'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                      'y', 'z',\n",
        "                      '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ],
      "metadata": {
        "id": "uYLpSNnljbZp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_eng = { i:c for i,c in enumerate(english_vocab)}\n",
        "eng_index = { c:i for i,c in enumerate(english_vocab)}\n",
        "index_hin = { i:c for i,c in enumerate(hindi_vocab)}\n",
        "hin_index = { c:i for i,c in enumerate(hindi_vocab)}\n",
        "print(len(hin_index))\n",
        "print(len(eng_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlY3SAZkjgkz",
        "outputId": "c4dfd02b-df0d-4499-e6bb-8d3abace6b5a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_file = '/content/drive/MyDrive/Translator_project/train.en'\n",
        "hin_file = '/content/drive/MyDrive/Translator_project/train.hi'\n",
        "\n",
        "def read_lines(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        sentence = file.readlines()\n",
        "        return sentence\n",
        "\n",
        "eng_sentence = read_lines(eng_file)\n",
        "hin_sentence = read_lines(hin_file)\n",
        "print(len(eng_sentence))\n",
        "# storing sentences to a list of max length\n",
        "max_len = 100000 #5 percent of dataset\n",
        "####1800000 should be here for full dataset\n",
        "eng_sentence = eng_sentence[:max_len]\n",
        "hin_sentence = hin_sentence[:max_len]"
      ],
      "metadata": {
        "id": "uLMd0fn7jihv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969026d1-abd0-4b71-9cb3-33f67f901bed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8568307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hin_sentence[-5:]"
      ],
      "metadata": {
        "id": "hNJjqqkMjnfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8004a261-a896-41fd-d832-2855b0291da5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['भारत वेस्टइंडीज के दौरे पर चार टेस्ट मैच खेलेगी।\\n',\n",
              " 'हाई ब्लड प्रेशर को कम करने के साथ अनिद्रा से निजात दिलाता है।\\n',\n",
              " \"राष्ट्रीय स्वयं सेवक संघ (आरएसएस) के नेता इंद्रेश कुमार का मानना है कि 'पश्चिमी' परंरपरा वेलेंटाइंस डे रेप, अवैध बच्चा और औरतों के ख़िलाफ़ हिंसा का लिए ज़िम्मेदार है.\\n\",\n",
              " 'पुलिस अधिकारी ने कहा कि इसके बारे में पूरी जानकारी पोस्टमार्टम की रिपोर्ट आने के बाद ही पता चल पायेगी।\\n',\n",
              " 'आईटी ग्रिड ऑफिस से मिले हार्ड डिस्कों के विश्लेषण को दौरान तेलंगाना स्टेट फॉरेंसिक साइंस लैबोरेटरी ने कहा कि कंपनी के पास तेलंगाना और आंध्र प्रदेश के 78,221,397 निवासियों के आधार से संबंधित आंकड़े हैं।\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentence = [sentence.rstrip('\\n').lower() for sentence in eng_sentence]\n",
        "hin_sentence = [sentence.rstrip('\\n').lower() for sentence in hin_sentence]\n",
        "print(len(hin_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHqMAkrakPc0",
        "outputId": "de49bb93-042e-47bc-8c0b-34d9c0ef4e2c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentence[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM4VlqmxkTIE",
        "outputId": "21f5fa9c-4f6e-4d4f-b0da-aa7a26fa52b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['india will play 4 tests against west indies',\n",
              " 'reduces high blood pressure and relieves insomnia.',\n",
              " '\"if rss leader indresh kumar is to be believed, the \"\"western\"\" tradition of valentine\\'s day is responsible for rape, illegitimate children, and violence against women.\"',\n",
              " 'police officials said that the details could be revealed after receiving the post-mortem report.',\n",
              " 'during analysis of hard disks recovered earlier from the it grids office, telangana state forensic science laboratory (tsfsl) experts say they found that the firm was in possession of 78,221,397 records of aadhaar data belonging to telangana and andhra pradesh.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def max_len(language):\n",
        "    P = 99\n",
        "    length = [len(i) for i in language]\n",
        "    maximum_len = max(length)\n",
        "    percentile =  np.percentile(length, P)\n",
        "    return maximum_len , percentile\n",
        "\n",
        "print('max lenght & Percentile of English Sentences:', max_len(eng_sentence))\n",
        "print('max lenght & Percentile of Hindi Sentences:', max_len(hin_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4yKzc_TkZfr",
        "outputId": "65216532-293c-4e11-d364-afee474816f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max lenght & Percentile of English Sentences: (1363, 353.0)\n",
            "max lenght & Percentile of Hindi Sentences: (1584, 354.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 360\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1)\n",
        "\n",
        "valid_sentence_len = []\n",
        "for index in range(len(hin_sentence)):\n",
        "    hindi_sentence, english_sentence = hin_sentence[index], eng_sentence[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocab):\n",
        "        valid_sentence_len.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hin_sentence)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_len)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1DjfdM5kbiI",
        "outputId": "6f94a642-5002-404d-8917-0d8ba795e48e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 100000\n",
            "Number of valid sentences: 8954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_sentence_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FzKzGVOkeug",
        "outputId": "b4270462-46d6-46c6-e791-2ee8f22feb7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7,\n",
              " 9,\n",
              " 23,\n",
              " 31,\n",
              " 35,\n",
              " 46,\n",
              " 52,\n",
              " 58,\n",
              " 71,\n",
              " 77,\n",
              " 78,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 102,\n",
              " 120,\n",
              " 129,\n",
              " 132,\n",
              " 138,\n",
              " 155,\n",
              " 172,\n",
              " 186,\n",
              " 216,\n",
              " 243,\n",
              " 249,\n",
              " 266,\n",
              " 282,\n",
              " 283,\n",
              " 297,\n",
              " 309,\n",
              " 326,\n",
              " 340,\n",
              " 348,\n",
              " 362,\n",
              " 377,\n",
              " 388,\n",
              " 424,\n",
              " 429,\n",
              " 450,\n",
              " 451,\n",
              " 462,\n",
              " 479,\n",
              " 496,\n",
              " 500,\n",
              " 502,\n",
              " 504,\n",
              " 515,\n",
              " 521,\n",
              " 535,\n",
              " 545,\n",
              " 549,\n",
              " 553,\n",
              " 567,\n",
              " 571,\n",
              " 577,\n",
              " 580,\n",
              " 585,\n",
              " 589,\n",
              " 628,\n",
              " 631,\n",
              " 647,\n",
              " 652,\n",
              " 659,\n",
              " 668,\n",
              " 690,\n",
              " 705,\n",
              " 742,\n",
              " 743,\n",
              " 750,\n",
              " 760,\n",
              " 779,\n",
              " 806,\n",
              " 815,\n",
              " 824,\n",
              " 830,\n",
              " 835,\n",
              " 843,\n",
              " 852,\n",
              " 857,\n",
              " 867,\n",
              " 870,\n",
              " 876,\n",
              " 896,\n",
              " 900,\n",
              " 929,\n",
              " 937,\n",
              " 945,\n",
              " 949,\n",
              " 968,\n",
              " 971,\n",
              " 972,\n",
              " 979,\n",
              " 982,\n",
              " 985,\n",
              " 992,\n",
              " 996,\n",
              " 1001,\n",
              " 1024,\n",
              " 1065,\n",
              " 1071,\n",
              " 1078,\n",
              " 1108,\n",
              " 1122,\n",
              " 1127,\n",
              " 1133,\n",
              " 1134,\n",
              " 1135,\n",
              " 1141,\n",
              " 1142,\n",
              " 1149,\n",
              " 1152,\n",
              " 1157,\n",
              " 1163,\n",
              " 1170,\n",
              " 1175,\n",
              " 1176,\n",
              " 1179,\n",
              " 1183,\n",
              " 1196,\n",
              " 1230,\n",
              " 1244,\n",
              " 1246,\n",
              " 1253,\n",
              " 1256,\n",
              " 1275,\n",
              " 1279,\n",
              " 1282,\n",
              " 1315,\n",
              " 1324,\n",
              " 1329,\n",
              " 1337,\n",
              " 1342,\n",
              " 1355,\n",
              " 1360,\n",
              " 1366,\n",
              " 1377,\n",
              " 1381,\n",
              " 1382,\n",
              " 1391,\n",
              " 1411,\n",
              " 1421,\n",
              " 1431,\n",
              " 1433,\n",
              " 1450,\n",
              " 1476,\n",
              " 1490,\n",
              " 1491,\n",
              " 1494,\n",
              " 1498,\n",
              " 1511,\n",
              " 1513,\n",
              " 1520,\n",
              " 1532,\n",
              " 1536,\n",
              " 1545,\n",
              " 1547,\n",
              " 1562,\n",
              " 1564,\n",
              " 1588,\n",
              " 1591,\n",
              " 1594,\n",
              " 1618,\n",
              " 1619,\n",
              " 1625,\n",
              " 1650,\n",
              " 1652,\n",
              " 1656,\n",
              " 1658,\n",
              " 1669,\n",
              " 1687,\n",
              " 1722,\n",
              " 1733,\n",
              " 1740,\n",
              " 1751,\n",
              " 1752,\n",
              " 1760,\n",
              " 1783,\n",
              " 1793,\n",
              " 1823,\n",
              " 1831,\n",
              " 1851,\n",
              " 1853,\n",
              " 1863,\n",
              " 1865,\n",
              " 1866,\n",
              " 1878,\n",
              " 1891,\n",
              " 1902,\n",
              " 1907,\n",
              " 1916,\n",
              " 1918,\n",
              " 1956,\n",
              " 1960,\n",
              " 1963,\n",
              " 1988,\n",
              " 1995,\n",
              " 2017,\n",
              " 2054,\n",
              " 2066,\n",
              " 2072,\n",
              " 2076,\n",
              " 2117,\n",
              " 2123,\n",
              " 2126,\n",
              " 2129,\n",
              " 2138,\n",
              " 2149,\n",
              " 2167,\n",
              " 2169,\n",
              " 2173,\n",
              " 2189,\n",
              " 2199,\n",
              " 2200,\n",
              " 2222,\n",
              " 2228,\n",
              " 2238,\n",
              " 2247,\n",
              " 2261,\n",
              " 2277,\n",
              " 2284,\n",
              " 2285,\n",
              " 2306,\n",
              " 2333,\n",
              " 2339,\n",
              " 2341,\n",
              " 2342,\n",
              " 2354,\n",
              " 2363,\n",
              " 2404,\n",
              " 2411,\n",
              " 2427,\n",
              " 2430,\n",
              " 2431,\n",
              " 2436,\n",
              " 2465,\n",
              " 2474,\n",
              " 2480,\n",
              " 2482,\n",
              " 2497,\n",
              " 2503,\n",
              " 2504,\n",
              " 2508,\n",
              " 2535,\n",
              " 2539,\n",
              " 2545,\n",
              " 2556,\n",
              " 2573,\n",
              " 2577,\n",
              " 2590,\n",
              " 2594,\n",
              " 2605,\n",
              " 2617,\n",
              " 2642,\n",
              " 2648,\n",
              " 2650,\n",
              " 2656,\n",
              " 2662,\n",
              " 2663,\n",
              " 2665,\n",
              " 2680,\n",
              " 2682,\n",
              " 2686,\n",
              " 2695,\n",
              " 2728,\n",
              " 2729,\n",
              " 2740,\n",
              " 2741,\n",
              " 2768,\n",
              " 2775,\n",
              " 2780,\n",
              " 2788,\n",
              " 2792,\n",
              " 2809,\n",
              " 2818,\n",
              " 2820,\n",
              " 2827,\n",
              " 2866,\n",
              " 2869,\n",
              " 2897,\n",
              " 2908,\n",
              " 2928,\n",
              " 2952,\n",
              " 2960,\n",
              " 2963,\n",
              " 2966,\n",
              " 2967,\n",
              " 2970,\n",
              " 2985,\n",
              " 3006,\n",
              " 3009,\n",
              " 3022,\n",
              " 3028,\n",
              " 3060,\n",
              " 3069,\n",
              " 3072,\n",
              " 3074,\n",
              " 3077,\n",
              " 3079,\n",
              " 3088,\n",
              " 3114,\n",
              " 3135,\n",
              " 3147,\n",
              " 3155,\n",
              " 3157,\n",
              " 3191,\n",
              " 3198,\n",
              " 3203,\n",
              " 3219,\n",
              " 3220,\n",
              " 3247,\n",
              " 3258,\n",
              " 3261,\n",
              " 3266,\n",
              " 3271,\n",
              " 3309,\n",
              " 3343,\n",
              " 3344,\n",
              " 3357,\n",
              " 3367,\n",
              " 3374,\n",
              " 3382,\n",
              " 3392,\n",
              " 3396,\n",
              " 3401,\n",
              " 3403,\n",
              " 3412,\n",
              " 3415,\n",
              " 3425,\n",
              " 3431,\n",
              " 3438,\n",
              " 3442,\n",
              " 3443,\n",
              " 3444,\n",
              " 3449,\n",
              " 3455,\n",
              " 3458,\n",
              " 3463,\n",
              " 3494,\n",
              " 3501,\n",
              " 3511,\n",
              " 3534,\n",
              " 3537,\n",
              " 3539,\n",
              " 3554,\n",
              " 3562,\n",
              " 3564,\n",
              " 3576,\n",
              " 3586,\n",
              " 3595,\n",
              " 3599,\n",
              " 3601,\n",
              " 3606,\n",
              " 3617,\n",
              " 3619,\n",
              " 3621,\n",
              " 3623,\n",
              " 3632,\n",
              " 3640,\n",
              " 3653,\n",
              " 3659,\n",
              " 3660,\n",
              " 3670,\n",
              " 3698,\n",
              " 3711,\n",
              " 3714,\n",
              " 3734,\n",
              " 3744,\n",
              " 3745,\n",
              " 3804,\n",
              " 3806,\n",
              " 3814,\n",
              " 3840,\n",
              " 3841,\n",
              " 3870,\n",
              " 3876,\n",
              " 3889,\n",
              " 3912,\n",
              " 3920,\n",
              " 3926,\n",
              " 3950,\n",
              " 3955,\n",
              " 3974,\n",
              " 3980,\n",
              " 3985,\n",
              " 3987,\n",
              " 4001,\n",
              " 4015,\n",
              " 4044,\n",
              " 4064,\n",
              " 4071,\n",
              " 4076,\n",
              " 4081,\n",
              " 4092,\n",
              " 4094,\n",
              " 4101,\n",
              " 4106,\n",
              " 4132,\n",
              " 4151,\n",
              " 4169,\n",
              " 4180,\n",
              " 4181,\n",
              " 4194,\n",
              " 4206,\n",
              " 4207,\n",
              " 4221,\n",
              " 4231,\n",
              " 4232,\n",
              " 4236,\n",
              " 4242,\n",
              " 4244,\n",
              " 4245,\n",
              " 4250,\n",
              " 4253,\n",
              " 4261,\n",
              " 4264,\n",
              " 4266,\n",
              " 4280,\n",
              " 4283,\n",
              " 4298,\n",
              " 4318,\n",
              " 4331,\n",
              " 4344,\n",
              " 4356,\n",
              " 4369,\n",
              " 4382,\n",
              " 4383,\n",
              " 4400,\n",
              " 4417,\n",
              " 4436,\n",
              " 4439,\n",
              " 4459,\n",
              " 4475,\n",
              " 4485,\n",
              " 4497,\n",
              " 4508,\n",
              " 4509,\n",
              " 4549,\n",
              " 4550,\n",
              " 4555,\n",
              " 4559,\n",
              " 4576,\n",
              " 4579,\n",
              " 4582,\n",
              " 4602,\n",
              " 4603,\n",
              " 4607,\n",
              " 4617,\n",
              " 4619,\n",
              " 4625,\n",
              " 4636,\n",
              " 4645,\n",
              " 4665,\n",
              " 4673,\n",
              " 4711,\n",
              " 4712,\n",
              " 4721,\n",
              " 4735,\n",
              " 4739,\n",
              " 4743,\n",
              " 4746,\n",
              " 4751,\n",
              " 4754,\n",
              " 4771,\n",
              " 4776,\n",
              " 4780,\n",
              " 4787,\n",
              " 4797,\n",
              " 4813,\n",
              " 4816,\n",
              " 4838,\n",
              " 4849,\n",
              " 4853,\n",
              " 4870,\n",
              " 4892,\n",
              " 4897,\n",
              " 4900,\n",
              " 4911,\n",
              " 4915,\n",
              " 4926,\n",
              " 4943,\n",
              " 4959,\n",
              " 4966,\n",
              " 4984,\n",
              " 4986,\n",
              " 5001,\n",
              " 5005,\n",
              " 5007,\n",
              " 5026,\n",
              " 5044,\n",
              " 5054,\n",
              " 5067,\n",
              " 5083,\n",
              " 5097,\n",
              " 5111,\n",
              " 5112,\n",
              " 5175,\n",
              " 5182,\n",
              " 5207,\n",
              " 5217,\n",
              " 5231,\n",
              " 5242,\n",
              " 5254,\n",
              " 5266,\n",
              " 5273,\n",
              " 5275,\n",
              " 5284,\n",
              " 5293,\n",
              " 5297,\n",
              " 5304,\n",
              " 5325,\n",
              " 5341,\n",
              " 5366,\n",
              " 5382,\n",
              " 5397,\n",
              " 5430,\n",
              " 5452,\n",
              " 5456,\n",
              " 5458,\n",
              " 5491,\n",
              " 5494,\n",
              " 5505,\n",
              " 5522,\n",
              " 5528,\n",
              " 5565,\n",
              " 5575,\n",
              " 5615,\n",
              " 5617,\n",
              " 5623,\n",
              " 5629,\n",
              " 5633,\n",
              " 5647,\n",
              " 5657,\n",
              " 5660,\n",
              " 5671,\n",
              " 5699,\n",
              " 5704,\n",
              " 5718,\n",
              " 5768,\n",
              " 5775,\n",
              " 5782,\n",
              " 5791,\n",
              " 5805,\n",
              " 5806,\n",
              " 5828,\n",
              " 5829,\n",
              " 5839,\n",
              " 5840,\n",
              " 5846,\n",
              " 5848,\n",
              " 5852,\n",
              " 5861,\n",
              " 5864,\n",
              " 5869,\n",
              " 5887,\n",
              " 5893,\n",
              " 5896,\n",
              " 5907,\n",
              " 5915,\n",
              " 5933,\n",
              " 5944,\n",
              " 5963,\n",
              " 5970,\n",
              " 5981,\n",
              " 5988,\n",
              " 5994,\n",
              " 5999,\n",
              " 6002,\n",
              " 6008,\n",
              " 6015,\n",
              " 6028,\n",
              " 6040,\n",
              " 6050,\n",
              " 6105,\n",
              " 6110,\n",
              " 6119,\n",
              " 6133,\n",
              " 6138,\n",
              " 6148,\n",
              " 6150,\n",
              " 6154,\n",
              " 6155,\n",
              " 6191,\n",
              " 6208,\n",
              " 6210,\n",
              " 6214,\n",
              " 6231,\n",
              " 6241,\n",
              " 6253,\n",
              " 6258,\n",
              " 6263,\n",
              " 6284,\n",
              " 6320,\n",
              " 6328,\n",
              " 6360,\n",
              " 6362,\n",
              " 6366,\n",
              " 6372,\n",
              " 6394,\n",
              " 6398,\n",
              " 6408,\n",
              " 6411,\n",
              " 6415,\n",
              " 6418,\n",
              " 6424,\n",
              " 6436,\n",
              " 6442,\n",
              " 6458,\n",
              " 6461,\n",
              " 6470,\n",
              " 6489,\n",
              " 6493,\n",
              " 6496,\n",
              " 6499,\n",
              " 6503,\n",
              " 6514,\n",
              " 6521,\n",
              " 6524,\n",
              " 6544,\n",
              " 6566,\n",
              " 6577,\n",
              " 6584,\n",
              " 6595,\n",
              " 6597,\n",
              " 6603,\n",
              " 6604,\n",
              " 6614,\n",
              " 6650,\n",
              " 6674,\n",
              " 6681,\n",
              " 6708,\n",
              " 6709,\n",
              " 6710,\n",
              " 6714,\n",
              " 6719,\n",
              " 6720,\n",
              " 6724,\n",
              " 6749,\n",
              " 6753,\n",
              " 6778,\n",
              " 6785,\n",
              " 6786,\n",
              " 6839,\n",
              " 6860,\n",
              " 6864,\n",
              " 6894,\n",
              " 6904,\n",
              " 6920,\n",
              " 6927,\n",
              " 6936,\n",
              " 6943,\n",
              " 6953,\n",
              " 6969,\n",
              " 6981,\n",
              " 6987,\n",
              " 6992,\n",
              " 6996,\n",
              " 7007,\n",
              " 7020,\n",
              " 7023,\n",
              " 7043,\n",
              " 7047,\n",
              " 7062,\n",
              " 7068,\n",
              " 7077,\n",
              " 7104,\n",
              " 7107,\n",
              " 7108,\n",
              " 7127,\n",
              " 7139,\n",
              " 7156,\n",
              " 7159,\n",
              " 7160,\n",
              " 7165,\n",
              " 7168,\n",
              " 7169,\n",
              " 7170,\n",
              " 7179,\n",
              " 7182,\n",
              " 7185,\n",
              " 7195,\n",
              " 7209,\n",
              " 7212,\n",
              " 7214,\n",
              " 7219,\n",
              " 7224,\n",
              " 7253,\n",
              " 7297,\n",
              " 7325,\n",
              " 7332,\n",
              " 7333,\n",
              " 7335,\n",
              " 7347,\n",
              " 7355,\n",
              " 7364,\n",
              " 7369,\n",
              " 7377,\n",
              " 7407,\n",
              " 7408,\n",
              " 7414,\n",
              " 7416,\n",
              " 7424,\n",
              " 7435,\n",
              " 7437,\n",
              " 7453,\n",
              " 7488,\n",
              " 7543,\n",
              " 7577,\n",
              " 7585,\n",
              " 7588,\n",
              " 7599,\n",
              " 7601,\n",
              " 7605,\n",
              " 7614,\n",
              " 7625,\n",
              " 7632,\n",
              " 7674,\n",
              " 7675,\n",
              " 7689,\n",
              " 7695,\n",
              " 7726,\n",
              " 7735,\n",
              " 7765,\n",
              " 7766,\n",
              " 7772,\n",
              " 7787,\n",
              " 7789,\n",
              " 7792,\n",
              " 7793,\n",
              " 7797,\n",
              " 7802,\n",
              " 7809,\n",
              " 7812,\n",
              " 7827,\n",
              " 7830,\n",
              " 7836,\n",
              " 7856,\n",
              " 7862,\n",
              " 7865,\n",
              " 7866,\n",
              " 7869,\n",
              " 7870,\n",
              " 7883,\n",
              " 7884,\n",
              " 7889,\n",
              " 7897,\n",
              " 7901,\n",
              " 7908,\n",
              " 7909,\n",
              " 7911,\n",
              " 7921,\n",
              " 7938,\n",
              " 7943,\n",
              " 7967,\n",
              " 7999,\n",
              " 8010,\n",
              " 8014,\n",
              " 8017,\n",
              " 8019,\n",
              " 8029,\n",
              " 8035,\n",
              " 8051,\n",
              " 8070,\n",
              " 8090,\n",
              " 8091,\n",
              " 8093,\n",
              " 8094,\n",
              " 8104,\n",
              " 8110,\n",
              " 8151,\n",
              " 8165,\n",
              " 8172,\n",
              " 8176,\n",
              " 8182,\n",
              " 8193,\n",
              " 8215,\n",
              " 8219,\n",
              " 8228,\n",
              " 8241,\n",
              " 8254,\n",
              " 8258,\n",
              " 8269,\n",
              " 8291,\n",
              " 8292,\n",
              " 8316,\n",
              " 8320,\n",
              " 8339,\n",
              " 8352,\n",
              " 8358,\n",
              " 8362,\n",
              " 8376,\n",
              " 8414,\n",
              " 8415,\n",
              " 8423,\n",
              " 8445,\n",
              " 8456,\n",
              " 8468,\n",
              " 8469,\n",
              " 8502,\n",
              " 8503,\n",
              " 8517,\n",
              " 8539,\n",
              " 8541,\n",
              " 8542,\n",
              " 8544,\n",
              " 8553,\n",
              " 8591,\n",
              " 8592,\n",
              " 8594,\n",
              " 8613,\n",
              " 8618,\n",
              " 8619,\n",
              " 8638,\n",
              " 8639,\n",
              " 8654,\n",
              " 8656,\n",
              " 8669,\n",
              " 8687,\n",
              " 8689,\n",
              " 8704,\n",
              " 8706,\n",
              " 8741,\n",
              " 8752,\n",
              " 8754,\n",
              " 8766,\n",
              " 8775,\n",
              " 8779,\n",
              " 8790,\n",
              " 8817,\n",
              " 8823,\n",
              " 8841,\n",
              " 8858,\n",
              " 8868,\n",
              " 8877,\n",
              " 8881,\n",
              " 8889,\n",
              " 8899,\n",
              " 8902,\n",
              " 8907,\n",
              " 8915,\n",
              " 8930,\n",
              " 8944,\n",
              " 8952,\n",
              " 8968,\n",
              " 8984,\n",
              " 8987,\n",
              " 8991,\n",
              " 9021,\n",
              " 9049,\n",
              " 9050,\n",
              " 9062,\n",
              " 9073,\n",
              " 9087,\n",
              " 9090,\n",
              " 9103,\n",
              " 9112,\n",
              " 9114,\n",
              " 9128,\n",
              " 9132,\n",
              " 9157,\n",
              " 9167,\n",
              " 9185,\n",
              " 9188,\n",
              " 9227,\n",
              " 9238,\n",
              " 9239,\n",
              " 9245,\n",
              " 9254,\n",
              " 9295,\n",
              " 9312,\n",
              " 9315,\n",
              " 9320,\n",
              " 9322,\n",
              " 9324,\n",
              " 9333,\n",
              " 9350,\n",
              " 9358,\n",
              " 9361,\n",
              " 9376,\n",
              " 9393,\n",
              " 9415,\n",
              " 9418,\n",
              " 9439,\n",
              " 9447,\n",
              " 9451,\n",
              " 9457,\n",
              " 9472,\n",
              " 9484,\n",
              " 9502,\n",
              " 9509,\n",
              " 9514,\n",
              " 9515,\n",
              " 9531,\n",
              " 9541,\n",
              " 9562,\n",
              " 9581,\n",
              " 9592,\n",
              " 9629,\n",
              " 9631,\n",
              " 9642,\n",
              " 9654,\n",
              " 9663,\n",
              " 9667,\n",
              " 9674,\n",
              " 9679,\n",
              " 9686,\n",
              " 9702,\n",
              " 9715,\n",
              " 9721,\n",
              " 9723,\n",
              " 9727,\n",
              " 9745,\n",
              " 9749,\n",
              " 9768,\n",
              " 9780,\n",
              " 9787,\n",
              " 9793,\n",
              " 9802,\n",
              " 9804,\n",
              " 9827,\n",
              " 9829,\n",
              " 9838,\n",
              " 9844,\n",
              " 9865,\n",
              " 9870,\n",
              " 9885,\n",
              " 9896,\n",
              " 9900,\n",
              " 9910,\n",
              " 9948,\n",
              " 9966,\n",
              " 9971,\n",
              " 9979,\n",
              " 9985,\n",
              " 9992,\n",
              " 10012,\n",
              " 10013,\n",
              " 10025,\n",
              " 10033,\n",
              " 10044,\n",
              " 10066,\n",
              " 10090,\n",
              " 10102,\n",
              " 10107,\n",
              " 10151,\n",
              " 10157,\n",
              " 10183,\n",
              " 10186,\n",
              " 10202,\n",
              " 10262,\n",
              " 10263,\n",
              " 10272,\n",
              " 10325,\n",
              " 10335,\n",
              " 10338,\n",
              " 10340,\n",
              " 10341,\n",
              " 10344,\n",
              " 10345,\n",
              " 10354,\n",
              " 10400,\n",
              " 10410,\n",
              " 10416,\n",
              " 10419,\n",
              " 10435,\n",
              " 10438,\n",
              " 10448,\n",
              " 10461,\n",
              " 10481,\n",
              " 10488,\n",
              " 10493,\n",
              " 10520,\n",
              " 10524,\n",
              " 10526,\n",
              " 10534,\n",
              " 10539,\n",
              " 10557,\n",
              " 10565,\n",
              " 10581,\n",
              " 10588,\n",
              " 10598,\n",
              " 10601,\n",
              " 10605,\n",
              " 10619,\n",
              " 10624,\n",
              " 10627,\n",
              " 10646,\n",
              " 10662,\n",
              " 10673,\n",
              " 10676,\n",
              " 10680,\n",
              " 10682,\n",
              " 10697,\n",
              " 10703,\n",
              " 10744,\n",
              " 10746,\n",
              " 10749,\n",
              " 10765,\n",
              " 10769,\n",
              " 10772,\n",
              " 10784,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_sentences = [hin_sentence[i] for i in valid_sentence_len]\n",
        "english_sentences = [eng_sentence[i] for i in valid_sentence_len]\n",
        "print(hindi_sentences[0])\n",
        "print(english_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEGrODOZkir0",
        "outputId": "84deeca5-a621-4f1c-bcc9-a74d79bdae64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "पुलिस मौके पर मौजूद है।\n",
            "police is present on the spot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "batch_size = 30\n",
        "#ffn_hidden is the number of  neurons in the hidden layer of Feed Forward Neural Network (Fully Connected)\n",
        "ffn_hidden = 2048\n",
        "num_head = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "# number of layers must be increased for accuracy\n",
        "max_sequence_length = 360\n",
        "d_output = len(hindi_vocab)\n",
        "\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                        ffn_hidden,\n",
        "                        num_head,\n",
        "                         drop_prob,\n",
        "                         num_layers,\n",
        "                         max_sequence_length,\n",
        "                         d_output,\n",
        "                          eng_index,\n",
        "                          hin_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ],
      "metadata": {
        "id": "bg3fxb-Ykknc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrNt8Fjkknod",
        "outputId": "cc050310-5f22-4129-e74c-d4b12e527717"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(71, 512)\n",
              "      (positional_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionWiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(102, 512)\n",
              "      (positional_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layer): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionWiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, english_sentences, hindi_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.hindi_sentences = hindi_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.english_sentences[index], self.hindi_sentences[index]"
      ],
      "metadata": {
        "id": "F4q9clHKkp4J"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(english_sentences, hindi_sentences)"
      ],
      "metadata": {
        "id": "TKLveNPbkuIR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Nd2n4qkv4G",
        "outputId": "39b37252-ad15-4bfc-aedd-b3ec28ab7aa4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8954"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[-5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqD8xkEEkxYj",
        "outputId": "6a490f54-3d72-4d1a-93ef-34a856aea77a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the films director ali abbas zafar shared the image on his twitter handle.',\n",
              " 'फिल्म के डायरेक्टर अली अब्बास जफर ने फिल्म का टाइटल और तस्वीर ट्विटर पर शेयर की है.')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ],
      "metadata": {
        "id": "muFXbKw9kzS8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOe4Pgfok2Wn",
        "outputId": "4e9f0c24-57d8-493e-df7f-4a5f3bda5a14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "299"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num>3:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfXX6tnRk5gt",
        "outputId": "ed570522-29c7-45f4-d0ca-9ff8610e5712"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('police is present on the spot.', 'it has a battery backup of 3050mah.', 'all our life as well as all our essential being is transformed into the possession of sachchidananda.', 'new delhi, 14th may, 2010', 'up and down', 'zenan, and hadashah, and migdal-gad,', 'that was nice of him, cahill said.', 'what is jcpoa?', 'make exercise a part of your daily routine.', 'the films premise is both interesting and exciting.', 'director: shyam benegal', 'the act of avoiding the liability of taxation.', 'so thats a good thing.', 'rajeev goyal will be appointed as cfo of afs.', 'the owner of the unidentified vehicle fled from the spot after the accident.', 'fcra comes into force w.e.f may 2011', 'the primary treatment is epinephrine.', \"you don 't say?\", 'woman dies during childbirth', 'if you pull a knife, you have to use ii.', 'during the raid they seized several documents.', 'priced from rs 40.', 'hes just a child.', 'forgive me, ma.', \"pm modi's advice\", 'i love playing tennis.', 'deserves promotion', 'pm: this is a national-level disaster.', 'submerged - arc welded (saw) pipes: -', 'the case will be heard on february 12'), ('पुलिस मौके पर मौजूद है।', 'और 3050एमएएच की दमदार बैटरी से लैस है।', 'हमारा समस्त जीवन तथा हमारी समस्त मूल सत्ता सच्चिदानन्द की प्राप्ति-रूप हो जाती है।', 'नई दिल्ली, 14 मई, 2010', 'ऊपर-नीचे', 'फिर सनान, हदाशा, मिगदलगाद,', 'काहिल ने कहा, यह शानदार था।', 'क्या है जेसीपीओए?', 'नियमित रूप से व्यायाम आदि को दिनचर्या का हिस्सा बनाने का।', 'फिल्म की कहानी काफी रोचक व दिलचस्प है।', 'पटकथा : श्याम बेनेगल', 'कराधान के दायित्व से बचने की कार्रवाई।', 'तो यह अच्छी बात है।', 'राजीव गोयल को एएफएस का सीएफओ नियुक्त किया जाएगा।', 'हादसे के बाद से अज्ञात वाहन चालक वाहन को लेकर मौके से फरार हो गया।', 'मई 2011 से एफसीआरए प्रभावी', 'इसका प्राथमिक उपचार एपिनेफ्रीन है।', 'क्या कहा!', 'महिला की प्रसव से पहले मौत', 'यदि चाकू निकाले, तो इस्तेमाल करना होगा.', 'छापेमारी के दौरान उसने कई दस्तावेज जब्त किए थे।', 'कीमत 40 रूपये है।', 'वह सिर्फ एक बच्चा है।', 'मम्मी मुझे माफ कर देना, मम्मी।', 'पीएम मोदी ने दिया ये सुझाव', 'मुझे टेनिस देखने का शौक है।', 'पदोन्नति पाने योग्य', 'यह राष्ट्रीय स्तर की आपदा है।', 'निमज्जित आर्क वेल्ड पाइप:-', 'इस मामले की अगली सुनवाई 12 अप्रैल को होगी।')]\n",
            "[('step 3: a pdf will be opened on the screen.', 'capital of cuba', 'but they help.', 'happy dhanteras!', 'it stands at 60.73% today', 'swiss start-up ecosystem is equally impressive.', 'cut it into four pieces.', 'pickles are considered very beneficial for health.', 'rabbit stew would make a nice change.', 'it can be anything.', 'an mou is executed with indian air force for crew selection and training', \"that required women to get their husband 's consent\", 'uddhav thackeray to be sworn-in as cm', 'what the law states:', 'which is appropriated, directly or indirectly, from, a state government undertaking by the state government.', 'the matter was heard on a day to day basis and spanned over for 40 days', 'what exactly, is hard to say.', 'the tables have turned.', 'cosmopolitan city', \"thus, the state is speedily moving towards being a 'power state' of the country.\", 'this is the super conscious state of atma.', 'its great for india.', 'the government is of the country.', 'it evolves with times and circumstances.', 'how much will be charged', 'the marine drive police station', 'it is a conducive day for travelling.', 'the film is being made under the banner of yash raj films.', 'the child got mentally disturbed after the incident.', 'the dsp said further action would be taken after getting the forensic report.'), ('चरण 3: आपकी स्क्रीन पर एक पीडीएफ खुल जाएगा।', 'हवाना', 'लेकिन उनकी मदद कीजिए।', 'शुभ धनतेरस !', 'यह आज 60.73 प्रतिशत है।', 'स्विस स्टार्ट-अप इकोसिस्टम भी उतना ही प्रभावशाली है।', 'इसके चार भाग करो.', 'खीरा खीरा हेल्थ के लिए काफी लाभकारी माना जाता है।', 'खरगोश को सझना एक नया बदलाव होगा।', 'यह कुछ भी हो सकता है.', 'चालक दल चयन और प्रशिक्षण के लिए भारतीय वायु सेना के साथ एक समझौता ज्ञापन का निष्पादन किया गया है', 'जिसके अनुसार किसी स्त्री को अपने पासपोर्ट के नवीनीकरण के लिए', 'सीएम पद के लिए उद्धव ठाकरे का नाम', 'क्या कहता है कानून:', '(आ) ऐसी कोर्इ रकम जो राज्य सरकार द्वारा किसी राज्य सरकार उपक्रम से प्रत्यक्ष या अप्रत्यक्ष रूप से विनियोजित की जाती है।', 'बीते 40 दिन से इस मामले की रोजाना सुनवाई हो रही है।', 'क्या सच्चाई है, कहना मुश्किल है।', 'सब कुछ बदल गया है।', 'विश्वनगर', \"इस प्रकार यह राज्य तेजी के साथ देश का '' ऊर्जा राज्य '' बनने की राह पर है।\", 'यही बुद्ध की करूणामयी देशना थी।', 'यह भारत के लिए शानदार है.', 'सरकार राज्य की होती है।', 'वक्त और हालात के अनुसार यह बदलता रहता है।', 'कितनी लगेगी फीस', 'मरीन ड्राइव पुलिस स्टेशन', 'घूमने जाने के लिए दिन अच्छा है।', 'ये यश राज फिल्म्स के बैनर तले बन रही है और इस फिल्म पर बहुत खर्च किया जा रहा है।', 'इस घटना के बाद बच्चा काफी परेशान हो गया।', 'डीएसपी ने कहा कि पोस्टमार्टम रिपोर्ट आने के बाद आगे की कार्रवाई की जाएगी.')]\n",
            "[(\"soor's used very easy, natural and explanatory language.\", 'his exterior is eye-catching.', 'the selection committee has taken the decision.', 'police have started investigation and lodged an fir.', 'bjp government is anti-farmer and anti-people.', 'the armed police has already arrived.', 'employer contribution - 3.83% of basic + da w.e.f. 01.04.2016.', 'his death is a personal loss for me.', 'new delhi, aug.28 (ani): u.s', 'he understood everything.', 'the work has been stopped.', 'good newwz is produced by karan johar.', 'a notification in this regard has also been issued by the state government.', 'the police has registered.', 'the meeting was chaired by district', 'immunity boost:', \"the tata nano was the result of ratan tata's vision of introducing the world's cheapest car'\", 'he is a prominent actor in telugu cinema.', 'he died due to heart attack.', 'this is likely to go up to 10% by 2030', '3 suspected wildlife poachers nabbed', 'after the accident the truck driver fled away.', 'the general state of healthcare in myanmar (also known as burma) is poor.', 'energy and its present use', 'jagdish sheth, literature and education, usa', 'the new century, thus, needs a new mindset.', 'the police is searching for them.', 'everybody can produce a video and upload it to youtube.', 'khortha dialect', 'after this, the encounter started.'), ('सूर की भाषा सरल स्वाभाविक तथा वाग्वैदिग्ध पूर्ण है।', 'इसकी बाहरी साजसज्जा मनमोहक है।', 'चयन समिति ने किया फैसला', 'पुलिस ने एफआईआर दर्ज कर कार्रवाई शुरू कर दी है।', 'भाजपा की नीति किसान विरोधी और जनविरोधी है।', 'सशस्त्र पुलिस आ चुकी है।', 'नियोक्ता का योगदान- मूल वेतन और डीए का 3.83 प्रतिशत जो 01.04.2016 से प्रभावी माना जाएगा।', 'उनका निधन मेरी निजी क्षति है।', 'नई दिल्ली, 18 अगस्त (आईएएनएस)।', 'वह देखते ही सब समझ गया।', 'जिससे काम रोक दिया गया।', 'करण जौहर ने गुड न्यूज फिल्म को प्रोड्यूस किया है.', 'राज्य सरकार ने इसके लिए एक विज्ञप्ति भी जारी की है।', 'पुलिस ने माला दर्ज कर लिया है।', 'बैठक की अध्यक्षता जिला', 'इम्युनिटी बढाए:', 'रतन टाटा के दुनिया की सबसे सस्ती कार लाने के सपने का परिणाम टाटा नैनो था।', 'वह एक लोकप्रिय तेलुगू अभिनेता थे।', 'हृदयगति रुकने की वजह से उनका निधन हो गया था।', 'ये योजना साल 2030 तक पूरी कर ली जाएगी', 'पशु तस्करी के 3 आरोपी काबू', 'हादसे के बाद पिकअप सवार भाग निकले।', '\"भारत की गरीब जनता (\"\"दरिद्रनारायण\"\") की सेवा ही उनका लक्ष्य था।\"', 'ऊर्जा और इसका वर्तमान उपयोग', 'जगदीश सेठ साहित्य-शिक्षा (अमेरिका)', 'नई सहस्त्राब्दि को आज एक नया बृद्विजीवी वर्ग चाहि।', 'पुलिस उनकी तलाश कर रही है .', 'यूट्यूब पर कोई भी किसी भी तरह का वीडियो बनाकर अपलोड कर सकता है.', 'खोरठा भाषा', 'इसके बाद उनके मिलने का सिलसिला चल निकला.')]\n",
            "[('thats what stimulates me.', 'manjit died during treatment.', 'well - pleased with their endeavour,', 'it has been communicated to government.', '\"\"\"you are but a human being like us. then bring us a sign if you are of the truthful.\"\"\"', 'this is going to set a good image of great britain.', 'she is allowed to do what she wants.', \"is it coins you want? i'll pay you.\", 'he had a base price of rs 20 lakh.', 'hbv goes through cycles of replication and non-replication.', 'three people were killed.', 'then let him call his associates.', 'he was elected twice from this seat.', 'this needs to be looked into seriously.', 'a bad husband, maybe.', 'after this the police took the accused into custody.', 'action should be taken against anybody who is found guilty.', '\"\"\"watch: sky ferreira performs on jimmy kimmel live!\"\".\"', 'right-channel', 'there is also a subsidy on kerosene supplied through the public distribution system (pds).', 'and they came.', 'the occipital bone is trapezoid in shape and curved on itself.', 'yamuna below danger mark', 'since then the seat has been vacant.', 'jagat mashay remained ever grateful.', 'fully charged.', 'fashion trailblazer award: marc jacobs', 'hey, let go of her!', 'he is being sent to jail.', 'job location : jammu & kashmir'), ('यही मुझे आकर्षित करती है।', 'उपचार के दौरान ममता की मौत हो गई।', 'अपने प्रयास पर प्रसन्न,', 'इसे शासन के पास भेज दिया गया है।', '\"तू बस हमारे ही जैसा एक आदमी है। यदि तू सच्चा है, तो कोई निशानी ले आ।\"\"\"', 'यह ब्रिटेन की बहुत अच्छी छवि पेश करने जा रहा है।', 'वह जो चाहता है उसे वह करने दिया जाए।', 'मुझे दे.', 'उनका 20 लाख का बेस प्राइस था।', 'एचबीवी प्रतिकृति और गैरप्रतिकृति के चक्र के माध्यम से चला जाता है।', 'तीन की मौत हो गई।', 'अब बुला ले वह अपनी मजलिस को!', 'तीन बार वह इस सीट से चुने गए।', 'इस पर व्यापकता से विचार किए जाने की जरूरत है.', 'एक बुरा पति, शायद.', 'जिसके बाद पुलिस ने आरोपी पति को गिरफ्तार कर जेल भेज दिया है।', 'जो भी दोषी है, उस पर तो कार्यवाही होनी चाहिए.', '\"\"\" अवतार के प्रचार के दौरान कैमरून ने जिमी किम्मल लाइव!\"', 'मोनो- चैनल', 'सार्वजनिक वितरण प्रणाली (पीडीएस) के माध्यम से आपूर्ति की जाने वाले मिट्टी के तेल पर भी सब्सिडी है.', 'और आये थे।', 'पश्चकपालास्थि विषम चर्तुभुज आकृति की होती है तथा स्वयमेव वक्र युक्त होती है', 'खतरे के निशान से नीचे यमुना', 'तब से यह कुर्सी खाली थी।', 'मेरी बदनामी होगी।', 'पूरी तरह से चार्ज.', 'मार्क जैकब्स को मिला फैशन ट्रेलब्लेजर अवार्ड मिला .', 'अरे, उसे जाने दो!', 'उसे जेल भेजने की कार्यवाही की जा रही है।', 'नौकरी का स्थान: जम्मू और कश्मीर')]\n",
            "[('for jobs.', 'the matter was...', \"don't help big man.\", 'and is as important to the well - being of the planet', 'the police, however, arrested the accused.', 'subsequently, they sexually assaulted him.', 'and the surrender / deletion certificate of the previous ration card, if there was any.', 'the film is expected to roll out soon.', 'this festival is also known as vijaydashmi.', 'rahman and others were present.', 'india had launched a diplomatic offensive against pakistan after the pulwama attack.', 'investing in bcd should now be sec - ond nature for the human family, as natural and inevitable to our lives as the sun and the rain on a field of rice.', 'the gateway is elaborate with intricately carved out embellishments', 'the body was shifted to osmania hospital for postmortem.', 'so that an algorithm can close the deal', 'but it is the narrow door that opens onto the road to everlasting life.', 'therein lies danger for jawaharlal and for india.', '9 chantal from canada', 'one seat remained vacant.', 'also, he writes poetry.', 'the reaction requires a metal catalyst.', 'beginning beginning', 'this is a hand driven machine.', 'default shadow', 'where did that come from?', 'that completely changed me.', 'william pitt', 'senior civil judges', 'pm modi expresses grief', 'my wife is cooking.'), ('नौकरी के लिए', 'यह था मामला .', 'बिग मैन की मदद मत करो.', 'तथा वह ग्रह को बेहतर बनाने के लिए उतने आवश्यक हो', 'लेकिन पुलिस आरोपी को बचा कर ले गई।', 'जिसके बाद उनके साथ यौन शोषण किया गया.', 'और पहले से राशन कार्ड यदि कोई हो तो उसको अभ्यर्पित करने/रद्द करने का प्रमाणपत्र जमा करने की आवश्यशकता है।', 'उम्मीद है जल्दी ही शुरू होगी फिल्म.', 'इस पर्व को विजय दशमी भी कहा जाता है।', 'हेमा रमन आदि मौजूद रहे।', 'पुलवामा हमले के बाद भारत ने पाकिस्तान के खिलाफ सख्त रुख अख्तियार कर लिया है.', 'यह हमारे जीवन की उतनी ही प्राकृतिक और अनिवार्य आवश्यकता है जितनी धान के खेत के लिए सूरज और वर्षा की आवश्यकता होती है।', 'प्रवेश द्वार जटिल नक्काशी से सुसज्जित है।', 'पोस्टमार्टेम के लिए शव उस्मानिया अस्पताल भेज दिया।', 'सिर्फ इसलिए की एक एल्गोरिथ्म सौदा कर सके', 'लेकिन सकेत द्वार ही अनन्त जीवन की ओर ले जानेवाले रास्ते पर खुल जाता है ।', 'उससे भारत और भारतीयता को खतरा है।', '9 कनाडा से शैनटल', 'एक सीट खाली रही।', 'और कविता भी लिखती है।', 'इस अभिक्रिय के लिए धातु उत्प्रेरक की आवश्यकता होती है ।', 'हुई थी थी', 'यह हस्तचलित मशीन है।', 'सजावट:', 'कहा से आई है?', 'इसने मुझे पूरी तरह से बदलकर रख दिया।', 'विलियम पिट', 'वरिष्ठ सिविल जज', 'पीएम मोदी ने जताया शोक:-', 'मेरी पत्नी पका रही है।')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to prevent the padding tokens from contributing to the loss.\n",
        "criterian = nn.CrossEntropyLoss( ignore_index = hin_index[PADDING_TOKEN]) #so that the loss calculation ignores cases where the target label is the padding token.\n",
        "\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1 : # initializing weight parameters for 2d matrix or more\n",
        "        nn.init.xavier_uniform(params)\n",
        "        # nn.init.xavier_uniform --> initializes the weights of the layer with values drawn from a uniform distribution within a specified range.\n",
        "        #  It helps in better weight initialization, which can lead to more stable training and better convergence of neural network models.\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr = 1e-4) # Adam Optimizer\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BP0Q-a4lFHQ",
        "outputId": "24bc6fd3-74be-4bb6-fcb4-2979ad1bc14d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-37c934e5bce7>:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_mask(eng_batch, hin_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal = 1)\n",
        "    encoder_padding_mask = torch.full([num_sentences,max_sequence_length, max_sequence_length], False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences,max_sequence_length, max_sequence_length], False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences,max_sequence_length, max_sequence_length], False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "        eng_sentence_length, hin_sentence_length = len(eng_batch[idx]), len(hin_batch[idx]) # length of each sentence in a batch character wise(letters)\n",
        "        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length) # indices needed to be padded for english sentences\n",
        "        hin_chars_to_padding_mask = np.arange(hin_sentence_length + 1, max_sequence_length)# indices needed to be padded for hindi sentences\n",
        "        # The padding mask is used to mask out certain positions in the input sequence where padding tokens are present\n",
        "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "        decoder_padding_mask_self_attention[idx, : ,hin_chars_to_padding_mask] = True\n",
        "        decoder_padding_mask_self_attention[idx, hin_chars_to_padding_mask, :] = True\n",
        "        decoder_padding_mask_cross_attention[idx, : ,eng_chars_to_padding_mask] = True\n",
        "        decoder_padding_mask_cross_attention[idx, hin_chars_to_padding_mask, :] = True\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ],
      "metadata": {
        "id": "l9WnBDrJlGoC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "transformer.train()\n",
        "#transformer.to(device)\n",
        "\n",
        "total_loss = 0\n",
        "num_epochs = 2\n",
        "# 10+ epoch should be there for model accuracy\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch', epoch)\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "        transformer.train()\n",
        "        eng_batch, hin_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_mask(eng_batch, hin_batch)\n",
        "        optim.zero_grad()\n",
        "        hin_predictions = transformer(eng_batch,\n",
        "                                      hin_batch,\n",
        "                                      encoder_self_attention_mask,\n",
        "                                      decoder_self_attention_mask,\n",
        "                                      decoder_cross_attention_mask,\n",
        "                                      encoder_start_token = False,\n",
        "                                      encoder_end_token = False,\n",
        "                                      decoder_start_token = True,\n",
        "                                      decoder_end_token = True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(hin_batch, start_token = False, end_token = True)\n",
        "\n",
        "        loss = criterian(hin_predictions.view(-1, d_output),labels.view(-1)) # -1 is a place holder\n",
        "        valid_indices = torch.where(labels.view(-1) == hin_index[PADDING_TOKEN], False, True) # <-- Binary Tensor\n",
        "        loss = loss.sum()/valid_indices.sum() # <-- calculating the mean loss over the valid examples.\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f'iteration {batch_num} : {loss.item()}')\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Hindi Translation: {hin_batch[0]}\")\n",
        "            hin_sentence_predicted = torch.argmax(hin_predictions[0], axis= 1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in hin_sentence_predicted:\n",
        "                if idx == hin_index[END_TOKEN]:\n",
        "                    break\n",
        "                predicted_sentence += index_hin[idx.item()]\n",
        "            print(f\"Hindi Prediction: {predicted_sentence}\")\n",
        "\n",
        "save_path = '/content/drive/MyDrive/transformer_weights.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': transformer.state_dict(),\n",
        "    'optimizer_state_dict': optim.state_dict()\n",
        "}, save_path)\n",
        "print('weights saved in drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_M6XYFalOOt",
        "outputId": "4f7dc799-e3a3-49b9-a352-ea5ad4535175"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/299 [00:13<1:07:07, 13.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 : 0.005238058045506477\n",
            "English: police is present on the spot.\n",
            "Hindi Translation: पुलिस मौके पर मौजूद है।\n",
            "Hindi Prediction: ेूेैैटटेै०ैैैखईैजटणईट0ेट९य९६ै९ईओट६0६षओषईय\"ऋ९ईैययईईईय९ैईईै९ई९ईई९ैै९ैै९घघईैैषैघयैेईेईझईैैैझईैैैैैयईैईईैैैाैैैैााैैैईैईईईतैमईईाईमैाईईईयैईईशययय्यईैैझ\"6घघश९९९९९ैई९९९ईईईघघघैघेओ<PADDING>झझझञञघईयेैैैैैैैेैईैै<PADDING><PADDING><PADDING>ॐैैैैमईऔेईईईेऔशोङऔईेेेईझगैैैैैई<START>टट.झझझझैैैैटैटकैणणैैफैैैझैैैैैईै<START>ैैैैैैैैैै\"\"भटैययऔ\"टझझझ<START>टयैाझैैझईझञङाईऔैैैैैैईैैझऔैटैैैैैटैट०ैफिटऔ५औभदटटटैझैैफफु9<START><START>फणईैैैैैैऔिैैैै ै\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 101/299 [18:28<36:01, 10.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 100 : 0.0032528347801417112\n",
            "English: cbse has also released an official notice regarding the same.\n",
            "Hindi Translation: इस बाबत सीबीएसई ने आधिकारिक घोषणा कर दी है.\n",
            "Hindi Prediction: ससर     क                  क       ा ा     ा        ा  ा  रा ा    र ्    ारककक \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/299 [36:18<17:19, 10.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200 : 0.0027133459225296974\n",
            "English: due to pollution,...\n",
            "Hindi Translation: प्रदूषण को देखते हुए .\n",
            "Hindi Prediction: इरक     का कक    ह \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 299/299 [53:34<00:00, 10.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/299 [00:09<47:16,  9.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 : 0.0030691162683069706\n",
            "English: police is present on the spot.\n",
            "Hindi Translation: पुलिस मौके पर मौजूद है।\n",
            "Hindi Prediction: इरस सेक र  कय क र   हा।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 101/299 [18:36<36:40, 11.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 100 : 0.0026950256433337927\n",
            "English: cbse has also released an official notice regarding the same.\n",
            "Hindi Translation: इस बाबत सीबीएसई ने आधिकारिक घोषणा कर दी है.\n",
            "Hindi Prediction: इसकसा   क       क  ककासा  काकर ा  हा का कै \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/299 [36:33<17:39, 10.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200 : 0.002319915685802698\n",
            "English: due to pollution,...\n",
            "Hindi Translation: प्रदूषण को देखते हुए .\n",
            "Hindi Prediction: इाराार  का का  ा है  ह\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 299/299 [54:19<00:00, 10.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights saved in drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/transformer_weights.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print(\"Model weights loaded.\")\n",
        "\n",
        "\n",
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  hin_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_mask(eng_sentence, hin_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              hin_sentence,\n",
        "                              encoder_self_attention_mask,\n",
        "                              decoder_self_attention_mask,\n",
        "                              decoder_cross_attention_mask,\n",
        "                              encoder_start_token=False,\n",
        "                              encoder_end_token=False,\n",
        "                              decoder_start_token=True,\n",
        "                              decoder_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_hin[next_token_index]\n",
        "    hin_sentence = (hin_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return hin_sentence[0]\n",
        "\n",
        "print(f\"Evaluation translation {eng_sentence} : {hin_sentence}\")\n",
        "print(\"---------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "xk1qisQelaeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835edd42-a015-4939-9a7e-b02ab34fe921"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "IYt1XeOLrdQd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"hi\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "OfdtBrairer9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611c7e00-503e-407e-fb90-fe0693871402"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "रो को कर का<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"how are you\")\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nmTFkpotPB7",
        "outputId": "df11be2b-191d-4749-cb4d-6d4fabcafb80"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "पुर्या के है।<END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def calculate_bleu_score(reference, candidate):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    return nltk.translate.bleu_score.sentence_bleu(reference, candidate)\n",
        "\n",
        "# Example usage\n",
        "reference_translation = \"आप कैसे हैं\"\n",
        "model_translation = \"पुर्या के है\"\n",
        "bleu_score = calculate_bleu_score(reference_translation, model_translation)\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_skwRtR2GHHh",
        "outputId": "c31854f5-2045-4c15-c4cb-11ca486bb901"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(reference, candidate):\n",
        "    correct = sum(1 for r, c in zip(reference, candidate) if r == c)\n",
        "    total = len(reference)\n",
        "    return correct / total\n",
        "\n",
        "def calculate_f1_score(reference, candidate):\n",
        "    true_positive = sum(1 for r, c in zip(reference, candidate) if r == c == 1)\n",
        "    false_positive = sum(1 for r, c in zip(reference, candidate) if r == 0 and c == 1)\n",
        "    false_negative = sum(1 for r, c in zip(reference, candidate) if r == 1 and c == 0)\n",
        "\n",
        "    precision = true_positive / (true_positive + false_positive)\n",
        "    recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "    return f1_score\n",
        "\n",
        "# Example usage\n",
        "reference_translation = [1, 2, 3, 4, 5]  # Replace with actual class labels\n",
        "model_translation = [1, 2, 3, 4, 5]  # Replace with actual predicted class labels\n",
        "\n",
        "accuracy = calculate_accuracy(reference_translation, model_translation)\n",
        "f1_score = calculate_f1_score(reference_translation, model_translation)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2k_3xBUGbmi",
        "outputId": "beb8e2fd-40c7-40e7-acae-09e3f4e603db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.34\n",
            "F1 Score: 0.48 \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
